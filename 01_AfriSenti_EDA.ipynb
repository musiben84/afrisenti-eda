{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41c010dc",
   "metadata": {},
   "source": [
    "# 01 â€” AfriSenti Twitter Sentiment: Initial EDA\n",
    "**Goals:** language distribution, text length analysis, and label imbalance. Saves plots/tables for your report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761c316f",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a949e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, sys, json, re\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 140\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "\n",
    "ROOT = Path.cwd().parents[0] if (Path.cwd().name == 'notebooks') else Path.cwd()\n",
    "DATA_DIR = ROOT / 'data'\n",
    "FIG_DIR = ROOT / 'outputs' / 'figures'\n",
    "TAB_DIR = ROOT / 'outputs' / 'tables'\n",
    "os.makedirs(FIG_DIR, exist_ok=True); os.makedirs(TAB_DIR, exist_ok=True)\n",
    "\n",
    "print('ROOT:', ROOT)\n",
    "print('Python:', sys.version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464a1723",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "Choose one: local CSVs in `../data/` or HuggingFace datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4452b58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Option A: Local CSVs\n",
    "def smart_read_csv(path):\n",
    "    try: return pd.read_csv(path)\n",
    "    except UnicodeDecodeError: return pd.read_csv(path, encoding='utf-8-sig')\n",
    "    except Exception: return pd.read_csv(path, sep='\\t')\n",
    "\n",
    "csvs = list(DATA_DIR.glob('*.csv'))\n",
    "if not csvs:\n",
    "    print('No CSVs found in data/. Please add dataset files.')\n",
    "    df = pd.DataFrame(columns=['text','label','lang'])\n",
    "else:\n",
    "    frames = []\n",
    "    for p in csvs:\n",
    "        d = smart_read_csv(p)\n",
    "        cols = {c.lower(): c for c in d.columns}\n",
    "        text_col = cols.get('text') or cols.get('tweet') or list(d.columns)[0]\n",
    "        label_col = cols.get('label') or cols.get('sentiment')\n",
    "        lang_col  = cols.get('lang') or cols.get('language')\n",
    "        tmp = pd.DataFrame()\n",
    "        tmp['text'] = d[text_col].astype(str)\n",
    "        tmp['label'] = d[label_col] if label_col else pd.NA\n",
    "        if lang_col: tmp['lang'] = d[lang_col].astype(str)\n",
    "        else:\n",
    "            name = p.stem.lower()\n",
    "            tmp['lang'] = 'sw' if 'sw' in name else ('am' if 'am' in name else ('en' if 'en' in name else 'unk'))\n",
    "        frames.append(tmp[['text','label','lang']])\n",
    "    df = pd.concat(frames, ignore_index=True)\n",
    "print('Loaded shape:', df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c370c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Option B: HuggingFace (edit dataset/config IDs and uncomment)\n",
    "# from datasets import load_dataset\n",
    "# ds_sw = load_dataset('Davlan/afrisent-semeval-2023', 'sw', split='train')\n",
    "# ds_am = load_dataset('Davlan/afrisent-semeval-2023', 'am', split='train')\n",
    "# ds_en = load_dataset('Davlan/afrisent-semeval-2023', 'en', split='train')\n",
    "# df = pd.concat([ds_sw.to_pandas().assign(lang='sw'),\n",
    "#                 ds_am.to_pandas().assign(lang='am'),\n",
    "#                 ds_en.to_pandas().assign(lang='en')], ignore_index=True)\n",
    "# label_map = {0:'negative', 1:'neutral', 2:'positive'}\n",
    "# if 'label' in df.columns and pd.api.types.is_numeric_dtype(df['label']):\n",
    "#     df['label'] = df['label'].map(label_map)\n",
    "# print('Loaded from HF:', df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549300fe",
   "metadata": {},
   "source": [
    "## 3. Clean & Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c547dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "orig_shape = df.shape\n",
    "missing = df.isna().sum()\n",
    "dups = df.duplicated(subset=['text','label','lang']).sum()\n",
    "print('Original:', orig_shape, '\\nMissing:\\n', missing, '\\nDuplicates:', dups)\n",
    "\n",
    "df = df.drop_duplicates(subset=['text','label','lang'], keep='first')\n",
    "df['text'] = df['text'].astype(str).str.replace('\\s+', ' ', regex=True).str.strip()\n",
    "if 'label' in df.columns: df['label'] = df['label'].astype(str)\n",
    "df['lang'] = df['lang'].astype(str)\n",
    "\n",
    "print('After dropping dupes:', df.shape)\n",
    "df.sample(5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a69e20",
   "metadata": {},
   "source": [
    "## 4. Language Distribution (Task 2a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7fb466",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lang_counts = df['lang'].value_counts().sort_index()\n",
    "print(lang_counts)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(x=lang_counts.index, y=lang_counts.values)\n",
    "plt.title('Language Distribution'); plt.xlabel('Language'); plt.ylabel('Count')\n",
    "plt.tight_layout(); plt.savefig(FIG_DIR/'lang_distribution.png', dpi=150); plt.show()\n",
    "\n",
    "lang_counts.to_csv(TAB_DIR/'lang_counts.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c664143",
   "metadata": {},
   "source": [
    "## 5. Text Length Analysis (Task 2b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb638fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['char_len'] = df['text'].str.len()\n",
    "df['tok_len'] = df['text'].str.split().apply(len)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1); plt.hist(df['char_len'], bins=50); plt.title('Char length')\n",
    "plt.subplot(1,2,2); plt.hist(df['tok_len'], bins=50); plt.title('Token length')\n",
    "plt.tight_layout(); plt.savefig(FIG_DIR/'length_hist_overall.png', dpi=150); plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "sns.boxplot(data=df, x='lang', y='tok_len')\n",
    "plt.title('Token length by language'); plt.xlabel('Language'); plt.ylabel('Tokens per tweet')\n",
    "plt.tight_layout(); plt.savefig(FIG_DIR/'length_box_by_lang.png', dpi=150); plt.show()\n",
    "\n",
    "df[['lang','char_len','tok_len']].groupby('lang').describe().to_csv(TAB_DIR/'length_stats_by_lang.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16167c7b",
   "metadata": {},
   "source": [
    "## 6. Label Imbalance (Task 2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10267b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'label' in df.columns and df['label'].notna().any():\n",
    "    label_counts = df['label'].value_counts().sort_index()\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.barplot(x=label_counts.index, y=label_counts.values)\n",
    "    plt.title('Overall Label Distribution'); plt.xlabel('Label'); plt.ylabel('Count')\n",
    "    plt.tight_layout(); plt.savefig(FIG_DIR/'label_distribution_overall.png', dpi=150); plt.show()\n",
    "\n",
    "    ctab = pd.crosstab(df['lang'], df['label']).sort_index()\n",
    "    ctab.to_csv(TAB_DIR/'label_by_lang_crosstab.csv')\n",
    "    ctab.plot(kind='bar', stacked=True, figsize=(8,4))\n",
    "    plt.title('Label Distribution by Language'); plt.xlabel('Language'); plt.ylabel('Count')\n",
    "    plt.tight_layout(); plt.savefig(FIG_DIR/'label_by_lang_stacked.png', dpi=150); plt.show()\n",
    "else:\n",
    "    print(\"No usable 'label' column; skipping label plots.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec4cd6d",
   "metadata": {},
   "source": [
    "## 7. Save Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4495c756",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summary = {\n",
    "    'original_shape': tuple(orig_shape),\n",
    "    'post_drop_duplicates': tuple(df.shape),\n",
    "    'missing': {k:int(v) for k,v in dict(missing).items()},\n",
    "    'duplicates_removed': int(dups),\n",
    "    'lang_counts': df['lang'].value_counts().to_dict()\n",
    "}\n",
    "if 'label' in df.columns and df['label'].notna().any():\n",
    "    summary['label_counts'] = df['label'].value_counts().to_dict()\n",
    "\n",
    "with open(TAB_DIR/'summary.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "print(json.dumps(summary, ensure_ascii=False, indent=2))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}