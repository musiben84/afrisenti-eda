{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AfriSenti — Multilingual Sentiment Analysis\n",
        "\n",
        "**GROUP -2 Assignment**  \n",
        "Sentiment Analysis (Multilingual Tweets)\n",
        "\n",
        "**Authors:** Ainedembe Denis, Musinguzi Benson  \n",
        "**Lecturer:** Dr. Sitenda Harriet\n",
        "\n",
        "This notebook implements a comprehensive analysis of the AfriSenti dataset, the largest sentiment analysis dataset for under-represented African languages, covering 110,000+ annotated tweets in 14 African languages.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports & Installs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Core packages installed\n",
            "\n",
            "All required libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install required packages for sentiment analysis assignment\n",
        "%pip install -q \"datasets<4.0.0\" pandas numpy\n",
        "%pip install -q matplotlib seaborn scikit-learn\n",
        "%pip install -q tqdm\n",
        "\n",
        "# Deep Learning frameworks (required for XLM-RoBERTa fine-tuning and LSTM baseline)\n",
        "# Note: May have compatibility issues with Python 3.13\n",
        "%pip install -q torch transformers\n",
        "\n",
        "print(\"Core packages installed\")\n",
        "\n",
        "# Data loading and manipulation\n",
        "from datasets import load_dataset, get_dataset_config_names\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualization (for data exploration, confusion matrix, attention visualization)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Evaluation metrics (F1-score, accuracy, ROC-AUC, confusion matrix)\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, roc_auc_score, \n",
        "    confusion_matrix, classification_report\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Deep Learning - PyTorch (for fine-tuning XLM-RoBERTa/AfriBERTa and LSTM)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "# Transformers (for mBERT, XLM-RoBERTa tokenizers and models)\n",
        "from transformers import (\n",
        "    AutoTokenizer, \n",
        "    AutoModelForSequenceClassification\n",
        ")\n",
        "\n",
        "# Progress bars\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"\\nAll required libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK data download skipped (may already be installed)\n"
          ]
        }
      ],
      "source": [
        "# Download NLTK data (run once)\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    nltk.download('vader_lexicon', quiet=True)\n",
        "    print(\"NLTK data downloaded\")\n",
        "except:\n",
        "    print(\"NLTK data download skipped (may already be installed)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Loading the Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available language configs: ['amh', 'hau', 'ibo', 'arq', 'ary', 'yor', 'por', 'twi', 'tso', 'tir', 'orm', 'pcm', 'kin', 'swa']\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['tweet', 'label'],\n",
            "        num_rows: 5984\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['tweet', 'label'],\n",
            "        num_rows: 1497\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['tweet', 'label'],\n",
            "        num_rows: 1999\n",
            "    })\n",
            "})\n",
            "{'tweet': 'Tesfaye ለካስ ጭብል ለብሰሽ የፕሮፌሰርን ፎቶ ለጥፈክ እልም ያልክ ባዳ ነክ እፈር ትንሽ', 'label': 2}\n"
          ]
        }
      ],
      "source": [
        "# List all available language configs for this dataset\n",
        "configs = get_dataset_config_names(\"HausaNLP/AfriSenti-Twitter\", trust_remote_code=True)\n",
        "print(\"Available language configs:\", configs)\n",
        "\n",
        "# Example: load Amharic (amh) with all splits (train/validation/test)\n",
        "amh_ds = load_dataset(\"HausaNLP/AfriSenti-Twitter\", \"amh\", trust_remote_code=True)\n",
        "print(amh_ds)\n",
        "\n",
        "# Example: load a single split (train) only\n",
        "amh_train = load_dataset(\"HausaNLP/AfriSenti-Twitter\", \"amh\", split=\"train\", trust_remote_code=True)\n",
        "print(amh_train[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Dataset Summary (size, languages, total tweets)\n",
        "\n",
        "This corresponds to the \"Dataset Summary\" section: 110k+ tweets, 14 languages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Language configs: ['amh', 'hau', 'ibo', 'arq', 'ary', 'yor', 'por', 'twi', 'tso', 'tir', 'orm', 'pcm', 'kin', 'swa']\n",
            "      train  validation  test  total\n",
            "lang                                \n",
            "amh    5984        1497  1999   9480\n",
            "hau   14172        2677  5303  22152\n",
            "ibo   10192        1841  3682  15715\n",
            "arq    1651         414   958   3023\n",
            "ary    5583         494  2961   9038\n",
            "yor    8522        2090  4515  15127\n",
            "por    3063         767  3662   7492\n",
            "twi    3481         388   949   4818\n",
            "tso     804         203   254   1261\n",
            "tir       0         398  2000   2398\n",
            "orm       0         396  2096   2492\n",
            "pcm    5121        1281  4154  10556\n",
            "kin    3302         827  1026   5155\n",
            "swa    1810         453   748   3011\n",
            "\n",
            "Total tweets across all languages: 111718\n"
          ]
        }
      ],
      "source": [
        "configs = get_dataset_config_names(\"HausaNLP/AfriSenti-Twitter\", trust_remote_code=True)\n",
        "print(\"Language configs:\", configs)\n",
        "\n",
        "summary_rows = []\n",
        "for cfg in configs:\n",
        "    # Load each split individually to handle cases where some splits don't exist\n",
        "    n_train = 0\n",
        "    n_val = 0\n",
        "    n_test = 0\n",
        "    \n",
        "    try:\n",
        "        train_ds = load_dataset(\"HausaNLP/AfriSenti-Twitter\", cfg, split=\"train\", trust_remote_code=True)\n",
        "        n_train = len(train_ds)\n",
        "    except ValueError:\n",
        "        n_train = 0\n",
        "    \n",
        "    try:\n",
        "        val_ds = load_dataset(\"HausaNLP/AfriSenti-Twitter\", cfg, split=\"validation\", trust_remote_code=True)\n",
        "        n_val = len(val_ds)\n",
        "    except ValueError:\n",
        "        n_val = 0\n",
        "    \n",
        "    try:\n",
        "        test_ds = load_dataset(\"HausaNLP/AfriSenti-Twitter\", cfg, split=\"test\", trust_remote_code=True)\n",
        "        n_test = len(test_ds)\n",
        "    except ValueError:\n",
        "        n_test = 0\n",
        "    \n",
        "    total = n_train + n_val + n_test\n",
        "    summary_rows.append({\n",
        "        \"lang\": cfg,\n",
        "        \"train\": n_train,\n",
        "        \"validation\": n_val,\n",
        "        \"test\": n_test,\n",
        "        \"total\": total\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_rows).set_index(\"lang\")\n",
        "print(summary_df)\n",
        "print(\"\\nTotal tweets across all languages:\", summary_df[\"total\"].sum())\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
